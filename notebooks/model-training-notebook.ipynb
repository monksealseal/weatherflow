{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WeatherFlow: Premier Flow-Matching Training Notebook\n",
        "\n",
        "This notebook is the recommended, end-to-end workflow for training WeatherFlow flow-matching models on real ERA5 reanalysis data. It stitches together production-ready components from the repository\u2014robust data loaders, physics-informed architectures, and research-grade trainers/solvers\u2014without any mock data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What you'll accomplish\n",
        "- Configure reproducible experiments for flow matching on ERA5 pressure-level data.\n",
        "- Build training/validation data loaders that stream consecutive timesteps for vector-field supervision.\n",
        "- Instantiate the **WeatherFlowMatch** model with physics-aware options and (optionally) spectral mixing.\n",
        "- Train with **FlowTrainer** (AMP, EMA, gradient clipping, and physics regularization supported) and track metrics.\n",
        "- Evaluate with the **WeatherODESolver** to roll out trajectories and compute science-relevant diagnostics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "- ERA5 pressure-level NetCDF files (downloadable via [CDS](https://cds.climate.copernicus.eu)) stored locally. Set `config[\"data_root\"]` to the folder that contains files like `era5_2016.nc`.\n",
        "- Dependencies listed in `notebooks/notebook_requirements.txt` (notably `torch`, `xarray`, `torchdiffeq`, `matplotlib`, `tqdm`).\n",
        "- GPU is recommended, but the notebook will automatically fall back to CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optional: Colab setup (clone repo and install deps)\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    repo_path = Path('/content/weatherflow')\n",
        "    if not repo_path.exists():\n",
        "        print('Cloning WeatherFlow repository for Colab...')\n",
        "        subprocess.run([\n",
        "            'git', 'clone', 'https://github.com/monksealseal/weatherflow.git', str(repo_path)\n",
        "        ], check=True)\n",
        "    else:\n",
        "        print('Repository already present at', repo_path)\n",
        "    os.chdir(repo_path)\n",
        "    print('Installing notebook dependencies...')\n",
        "    subprocess.run([\n",
        "        sys.executable, '-m', 'pip', 'install', '-r', 'notebooks/notebook_requirements.txt'\n",
        "    ], check=False)\n",
        "    os.environ['WEATHERFLOW_REPO'] = str(repo_path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Environment & repository setup\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "\n",
        "def resolve_repo_root() -> Path:\n",
        "    env_root = os.environ.get(\"WEATHERFLOW_REPO\")\n",
        "    if env_root:\n",
        "        return Path(env_root).resolve()\n",
        "\n",
        "    cwd = Path.cwd().resolve()\n",
        "\n",
        "    if (cwd / \"weatherflow\").exists():\n",
        "        return cwd\n",
        "    if (cwd / \"notebooks\").exists() and (cwd.parent / \"weatherflow\").exists():\n",
        "        return cwd.parent\n",
        "    if (cwd.parent / \"weatherflow\").exists():\n",
        "        return cwd.parent\n",
        "\n",
        "    return cwd\n",
        "\n",
        "REPO_ROOT = resolve_repo_root()\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(REPO_ROOT))\n",
        "\n",
        "# Report basic environment info\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Repository root: {REPO_ROOT}\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Torch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Experiment configuration\n",
        "from weatherflow.training.utils import set_global_seed\n",
        "\n",
        "set_global_seed(42)\n",
        "\n",
        "def default_data_root() -> str:\n",
        "    # Keep raw data outside the repo by default\n",
        "    candidate = Path(os.environ.get(\"WEATHERFLOW_DATA\", \"~/.cache/weatherflow/era5\")).expanduser()\n",
        "    candidate.mkdir(parents=True, exist_ok=True)\n",
        "    return str(candidate)\n",
        "\n",
        "config = {\n",
        "    # Data\n",
        "    \"data_root\": default_data_root(),\n",
        "    \"train_years\": [2016, 2017],\n",
        "    \"val_years\": [2018],\n",
        "    \"variables\": [\n",
        "        \"geopotential\",\n",
        "        \"temperature\",\n",
        "        \"u_component_of_wind\",\n",
        "        \"v_component_of_wind\",\n",
        "    ],\n",
        "    \"pressure_levels\": [500, 700],\n",
        "    \"batch_size\": 4,\n",
        "    \"num_workers\": 2,\n",
        "    \"download_missing\": False,  # Set to True to fetch data via CDSAPI\n",
        "\n",
        "    # Model\n",
        "    \"hidden_dim\": 192,\n",
        "    \"n_layers\": 6,\n",
        "    \"use_attention\": True,\n",
        "    \"spherical_padding\": True,\n",
        "    \"use_graph_mp\": False,\n",
        "    \"use_spectral_mixer\": True,\n",
        "    \"spectral_modes\": 12,\n",
        "    \"physics_informed\": True,\n",
        "    \"enhanced_physics_losses\": True,\n",
        "\n",
        "    # Training\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"weight_decay\": 1e-2,\n",
        "    \"epochs\": 3,\n",
        "    \"loss_type\": \"mse\",\n",
        "    \"loss_weighting\": \"time\",\n",
        "    \"use_amp\": True,\n",
        "    \"physics_regularization\": True,\n",
        "    \"physics_lambda\": 0.1,\n",
        "    \"grad_clip\": 1.0,\n",
        "    \"ema_decay\": 0.999,\n",
        "\n",
        "    # Checkpoints & logging\n",
        "    \"checkpoint_dir\": str(REPO_ROOT / \"artifacts\" / \"checkpoints\"),\n",
        "    \"history_path\": str(REPO_ROOT / \"artifacts\" / \"training_history.json\"),\n",
        "}\n",
        "\n",
        "Path(config[\"checkpoint_dir\"]).mkdir(parents=True, exist_ok=True)\n",
        "Path(config[\"history_path\"]).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(json.dumps(config, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build ERA5 training/validation loaders\n",
        "We load real ERA5 pressure-level data and reshape consecutive timesteps into `(x0, x1)` pairs for vector-field supervision. The helper below respects the repository's normalization logic and keeps channel ordering consistent with the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from weatherflow.data.era5 import ERA5Dataset\n",
        "\n",
        "class ConsecutiveERA5Pairs(Dataset):\n",
        "    # Wrap an ERA5Dataset to return consecutive normalized frames\n",
        "\n",
        "    def __init__(self, base_dataset: ERA5Dataset):\n",
        "        if len(base_dataset) < 2:\n",
        "            raise ValueError(\"ERA5 dataset must contain at least two timesteps for pairing.\")\n",
        "        self.base = base_dataset\n",
        "\n",
        "    @staticmethod\n",
        "    def _flatten_channels(sample):\n",
        "        # Convert [variables, levels, lat, lon] -> [channels, lat, lon]\n",
        "        if sample.ndim != 4:\n",
        "            raise ValueError(f\"Expected 4D tensor, got {sample.shape}\")\n",
        "        vars_, levels, lat, lon = sample.shape\n",
        "        return sample.view(vars_ * levels, lat, lon)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.base) - 1\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        x0 = self._flatten_channels(self.base[idx])\n",
        "        x1 = self._flatten_channels(self.base[idx + 1])\n",
        "        return x0, x1\n",
        "\n",
        "\n",
        "def build_loaders(cfg):\n",
        "    train_raw = ERA5Dataset(\n",
        "        root_dir=cfg[\"data_root\"],\n",
        "        years=cfg[\"train_years\"],\n",
        "        variables=cfg[\"variables\"],\n",
        "        levels=cfg[\"pressure_levels\"],\n",
        "        download=cfg[\"download_missing\"],\n",
        "    )\n",
        "\n",
        "    val_raw = ERA5Dataset(\n",
        "        root_dir=cfg[\"data_root\"],\n",
        "        years=cfg[\"val_years\"],\n",
        "        variables=cfg[\"variables\"],\n",
        "        levels=cfg[\"pressure_levels\"],\n",
        "        download=cfg[\"download_missing\"],\n",
        "    )\n",
        "\n",
        "    train_ds = ConsecutiveERA5Pairs(train_raw)\n",
        "    val_ds = ConsecutiveERA5Pairs(val_raw)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=cfg[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=cfg[\"num_workers\"],\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=cfg[\"batch_size\"],\n",
        "        shuffle=False,\n",
        "        num_workers=cfg[\"num_workers\"],\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    # Inspect a single batch to derive channel/grid shapes for the model\n",
        "    sample_x0, _ = next(iter(train_loader))\n",
        "    data_shape = sample_x0.shape  # [batch, channels, lat, lon]\n",
        "\n",
        "    return train_loader, val_loader, data_shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "train_loader, val_loader, data_shape = build_loaders(config)\n",
        "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n",
        "print(f\"Batch shape: {data_shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Optional) quick visual check\n",
        "Use the `visualize` helper to denormalize and plot a slice of the raw ERA5 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Uncomment to view a single field (uses real ERA5 data)\n",
        "# _ = train_loader.dataset.base.visualize(idx=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the WeatherFlowMatch model\n",
        "We size the network directly from the data loader: the channel dimension equals `len(variables) * len(pressure_levels)`, and the grid size comes from the ERA5 latitude/longitude resolution. Physics-aware options (divergence regularization, enhanced physics losses, spectral mixing) are enabled to match our strongest research configurations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from weatherflow.models.flow_matching import WeatherFlowMatch\n",
        "\n",
        "input_channels = int(data_shape[1])\n",
        "grid_size = (int(data_shape[2]), int(data_shape[3]))\n",
        "\n",
        "model = WeatherFlowMatch(\n",
        "    input_channels=input_channels,\n",
        "    hidden_dim=config[\"hidden_dim\"],\n",
        "    n_layers=config[\"n_layers\"],\n",
        "    use_attention=config[\"use_attention\"],\n",
        "    grid_size=grid_size,\n",
        "    physics_informed=config[\"physics_informed\"],\n",
        "    window_size=8,\n",
        "    static_channels=0,\n",
        "    forcing_dim=0,\n",
        "    spherical_padding=config[\"spherical_padding\"],\n",
        "    use_graph_mp=config[\"use_graph_mp\"],\n",
        "    use_spectral_mixer=config[\"use_spectral_mixer\"],\n",
        "    spectral_modes=config[\"spectral_modes\"],\n",
        "    enhanced_physics_losses=config[\"enhanced_physics_losses\"],\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model parameters: {num_params/1e6:.2f}M\")\n",
        "print(f\"Grid size: {grid_size}, Channels: {input_channels}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trainer, optimizer, and scheduler\n",
        "`FlowTrainer` brings flow-matching losses, optional physics regularization, AMP, gradient clipping, EMA, and checkpointing in one place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from weatherflow.training.flow_trainer import FlowTrainer\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config[\"learning_rate\"],\n",
        "    weight_decay=config[\"weight_decay\"],\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        ")\n",
        "\n",
        "trainer = FlowTrainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    use_amp=config[\"use_amp\"],\n",
        "    use_wandb=False,\n",
        "    checkpoint_dir=config[\"checkpoint_dir\"],\n",
        "    scheduler=scheduler,\n",
        "    physics_regularization=config[\"physics_regularization\"],\n",
        "    physics_lambda=config[\"physics_lambda\"],\n",
        "    loss_type=config[\"loss_type\"],\n",
        "    loss_weighting=config[\"loss_weighting\"],\n",
        "    grad_clip=config[\"grad_clip\"],\n",
        "    ema_decay=config[\"ema_decay\"],\n",
        "    seed=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop\n",
        "This loop keeps a compact record of training/validation metrics, steps the scheduler, and writes checkpoints after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "history = []\n",
        "\n",
        "for epoch in range(config[\"epochs\"]):\n",
        "    trainer.current_epoch = epoch + 1\n",
        "\n",
        "    train_metrics = trainer.train_epoch(train_loader)\n",
        "    val_metrics = trainer.validate(val_loader)\n",
        "\n",
        "    # Scheduler step (ReduceLROnPlateau expects a validation metric)\n",
        "    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "        scheduler.step(val_metrics[\"val_loss\"])\n",
        "\n",
        "    epoch_record = {\n",
        "        \"epoch\": epoch + 1,\n",
        "        **train_metrics,\n",
        "        **val_metrics,\n",
        "        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "    }\n",
        "    history.append(epoch_record)\n",
        "\n",
        "    checkpoint_name = f\"epoch_{epoch + 1}.pt\"\n",
        "    trainer.save_checkpoint(checkpoint_name)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch + 1}: train_loss={train_metrics['loss']:.4f}, \"\n",
        "        f\"val_loss={val_metrics['val_loss']:.4f}, lr={epoch_record['lr']:.2e}\"\n",
        "    )\n",
        "\n",
        "# Persist history for downstream analysis\n",
        "with open(config[\"history_path\"], \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss curves\n",
        "Plotting training and validation losses helps spot divergence or underfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_losses = [h[\"loss\"] for h in history]\n",
        "val_losses = [h[\"val_loss\"] for h in history]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train Loss\")\n",
        "plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Flow-matching loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Roll out a trajectory with the Weather ODE Solver\n",
        "Use the trained vector field to integrate an initial state forward in time while enforcing physics-aware constraints. Metrics compare the final rollout against the true next-step target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from weatherflow.solvers.ode_solver import WeatherODESolver\n",
        "from weatherflow.training.metrics import rmse, mae, energy_ratio\n",
        "\n",
        "model.eval()\n",
        "solver = WeatherODESolver(\n",
        "    rtol=1e-5,\n",
        "    atol=1e-5,\n",
        "    physics_constraints=True,\n",
        "    constraint_types=[\"mass\", \"energy\", \"vorticity\"],\n",
        ")\n",
        "\n",
        "# Take a single validation batch\n",
        "val_batch = next(iter(val_loader))\n",
        "x0, x1 = (tensor.to(device) for tensor in val_batch)\n",
        "\n",
        "# Time grid from 0 -> 1 (can be aligned with your lead time convention)\n",
        "t_span = torch.linspace(0, 1, steps=5, device=device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    def velocity_fn(x, t_scalar):\n",
        "        # t_scalar from solver is a scalar tensor; expand to batch\n",
        "        t_batch = torch.full((x.shape[0],), float(t_scalar), device=device)\n",
        "        return model(x, t_batch)\n",
        "\n",
        "    rollout, solver_stats = solver.solve(velocity_fn, x0, t_span)\n",
        "\n",
        "pred_future = rollout[-1]\n",
        "\n",
        "print(\"Solver stats:\", solver_stats)\n",
        "print(\n",
        "    f\"RMSE vs target: {rmse(pred_future, x1).item():.4f}, \"\n",
        "    f\"MAE: {mae(pred_future, x1).item():.4f}, \"\n",
        "    f\"Energy ratio: {energy_ratio(pred_future, x1).item():.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "- Increase `epochs`, adjust `learning_rate`, or expand `train_years` for larger runs.\n",
        "- Toggle `use_spectral_mixer` / `spectral_modes` and `enhanced_physics_losses` to explore ablations.\n",
        "- Integrate with [Weights & Biases](https://wandb.ai/) by setting `use_wandb=True` in the `FlowTrainer` constructor for richer experiment tracking.\n",
        "- Export checkpoints from `artifacts/checkpoints/` for downstream evaluation notebooks."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}